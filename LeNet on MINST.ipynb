{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: LeNet on MINST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the shape of new data and see that our images are 28×28 pixels, so we need to add a new axis, which will represent a number of channels. Also, it is important to do one-hot encoding of labels and normalization of input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(28, 28) image shape\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_train[0].shape, 'image shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(28, 28, 1) image shape\n"
     ]
    }
   ],
   "source": [
    "# Add a new axis\n",
    "x_train = x_train[:, :, :, np.newaxis]\n",
    "x_test = x_test[:, :, :, np.newaxis]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_train[0].shape, 'image shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start using TensorFlow 2.0 in order to build our convolutional neural network. The easiest way to do this is by using the Sequential API. We will wrap it in a class called LeNet. The input is an image, and the output will be a class probability vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 model\n",
    "class LeNet(Sequential):\n",
    "    def __init__(self, input_shape, nb_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding=\"same\"))\n",
    "        self.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "        self.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "        self.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "        self.add(Flatten())\n",
    "        self.add(Dense(120, activation='tanh'))\n",
    "        self.add(Dense(84, activation='tanh'))\n",
    "        self.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer='adam',\n",
    "                    loss=categorical_crossentropy,\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"le_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LeNet(x_train[0].shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a model, we need to train its parameters to make it powerful. Let’s train the model for a given number of epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the logs in a timestamped subdirectory\n",
    "# This allows to easy select different training runs\n",
    "# In order not to overwrite some data, it is useful to have a name with a timestamp\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Specify the callback object\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# tf.keras.callback.TensorBoard ensures that logs are created and stored\n",
    "# We need to pass callback object to the fit method\n",
    "# The way to do this is by passing the list of callback objects, which is in our case just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "  1/938 [..............................] - ETA: 0s - loss: 2.3381 - accuracy: 0.1250WARNING:tensorflow:From /Users/zhangmaoyu/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.5559 - accuracy: 0.7965 - val_loss: 0.4528 - val_accuracy: 0.8312\n",
      "Epoch 2/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.3846 - accuracy: 0.8594 - val_loss: 0.3856 - val_accuracy: 0.8604\n",
      "Epoch 3/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.3413 - accuracy: 0.8741 - val_loss: 0.3627 - val_accuracy: 0.8683\n",
      "Epoch 4/20\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.3142 - accuracy: 0.8847 - val_loss: 0.3430 - val_accuracy: 0.8770\n",
      "Epoch 5/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2923 - accuracy: 0.8924 - val_loss: 0.3298 - val_accuracy: 0.8795\n",
      "Epoch 6/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2764 - accuracy: 0.8978 - val_loss: 0.3231 - val_accuracy: 0.8816\n",
      "Epoch 7/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2626 - accuracy: 0.9018 - val_loss: 0.3176 - val_accuracy: 0.8828\n",
      "Epoch 8/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.2478 - accuracy: 0.9089 - val_loss: 0.3230 - val_accuracy: 0.8825\n",
      "Epoch 9/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.2369 - accuracy: 0.9118 - val_loss: 0.3134 - val_accuracy: 0.8876\n",
      "Epoch 10/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.2250 - accuracy: 0.9158 - val_loss: 0.3188 - val_accuracy: 0.8828\n",
      "Epoch 11/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.2148 - accuracy: 0.9206 - val_loss: 0.3130 - val_accuracy: 0.8897\n",
      "Epoch 12/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.2055 - accuracy: 0.9225 - val_loss: 0.2954 - val_accuracy: 0.8947\n",
      "Epoch 13/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.1962 - accuracy: 0.9261 - val_loss: 0.3038 - val_accuracy: 0.8917\n",
      "Epoch 14/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.1879 - accuracy: 0.9302 - val_loss: 0.3081 - val_accuracy: 0.8906\n",
      "Epoch 15/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.1806 - accuracy: 0.9321 - val_loss: 0.3120 - val_accuracy: 0.8933\n",
      "Epoch 16/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.1714 - accuracy: 0.9371 - val_loss: 0.3092 - val_accuracy: 0.8938\n",
      "Epoch 17/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.1648 - accuracy: 0.9392 - val_loss: 0.3152 - val_accuracy: 0.8943\n",
      "Epoch 18/20\n",
      "938/938 [==============================] - 10s 10ms/step - loss: 0.1557 - accuracy: 0.9428 - val_loss: 0.3115 - val_accuracy: 0.8946\n",
      "Epoch 19/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.1505 - accuracy: 0.9437 - val_loss: 0.3096 - val_accuracy: 0.8962\n",
      "Epoch 20/20\n",
      "938/938 [==============================] - 10s 11ms/step - loss: 0.1419 - accuracy: 0.9475 - val_loss: 0.3219 - val_accuracy: 0.8918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb9cafe70a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_test, y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5438153f95a9018d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5438153f95a9018d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next,we will build Lenet by karas to compare our own lenet by training accuracy and loss,and testing accuracy and loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using karas to define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the digit dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Spliting data into train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "def create_model():\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Flatten - Flattens the input. Does not affect the batch size.\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "    # Dense - Fully conenected layer ( Each Input Neuron is connected to the output Neuron)\n",
    "    model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "    # Dropout is a technique used to improve over-fit on neural networks\n",
    "    # Basically during training some of neurons on a particular layer will be deactivated.\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "training_epochs = 20 # Total number of training epochs\n",
    "learning_rate = 0.001 # The learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "\n",
    "# Configure model for training\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "   1/1875 [..............................] - ETA: 0s - loss: 2.4593 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0023s vs `on_train_batch_end` time: 0.0182s). Check your callbacks.\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2209 - accuracy: 0.9356 - val_loss: 0.1136 - val_accuracy: 0.9647\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0962 - accuracy: 0.9711 - val_loss: 0.0745 - val_accuracy: 0.9763\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0694 - accuracy: 0.9787 - val_loss: 0.0775 - val_accuracy: 0.9759\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0523 - accuracy: 0.9836 - val_loss: 0.0692 - val_accuracy: 0.9801\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0436 - accuracy: 0.9860 - val_loss: 0.0643 - val_accuracy: 0.9800\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.0738 - val_accuracy: 0.9794\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0304 - accuracy: 0.9897 - val_loss: 0.0630 - val_accuracy: 0.9799\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0271 - accuracy: 0.9906 - val_loss: 0.0676 - val_accuracy: 0.9822\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0254 - accuracy: 0.9916 - val_loss: 0.0681 - val_accuracy: 0.9834\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0229 - accuracy: 0.9920 - val_loss: 0.0639 - val_accuracy: 0.9831\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0198 - accuracy: 0.9933 - val_loss: 0.0741 - val_accuracy: 0.9827\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0194 - accuracy: 0.9934 - val_loss: 0.0700 - val_accuracy: 0.9826\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.0815 - val_accuracy: 0.9815\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0153 - accuracy: 0.9946 - val_loss: 0.0765 - val_accuracy: 0.9831\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.0855 - val_accuracy: 0.9833\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0157 - accuracy: 0.9944 - val_loss: 0.0824 - val_accuracy: 0.9833\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.0903 - val_accuracy: 0.9836\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.1097 - val_accuracy: 0.9784\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0942 - val_accuracy: 0.9819\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0129 - accuracy: 0.9956 - val_loss: 0.0939 - val_accuracy: 0.9823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f815206f2e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order not to overwrite some data, place the logs in a timestamped subdirectory\n",
    "# This allows to easy select different training runs\n",
    "log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "# Specify the callback object\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# tf.keras.callback.TensorBoard ensures that logs are created and stored \n",
    "# We need to pass callback object to the fit method\n",
    "# The way to do this is by passing the list of callback objects, which is in our case just one\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=20, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 15207), started 0:14:03 ago. (Use '!kill 15207' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-89988b6d85ef8af7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-89988b6d85ef8af7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last command will open a TensorBoard in our notebook，So here, we have visualized all the graphs together, including train and validation accuracy and loss over epochs on x-axis, and actual value on y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of creating TensorBoard records using the Sequential API is that we can easily visualize our model. \n",
    "For this, click on Graphs tab. By default, we will see an op-level graph, which helps us to understand how TensorFlow understands our program. Examining the op-level graph can give us insight as how to change our model.\n",
    "On the Distributions dashboard. Here we can view the distribution of each weight. The lighter part shows all the weight across time and the shaded part shows weights that are actually activated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using networks built by Karas, we can see that our own networks are performing well,and not bad for only 20 epochs.The training accuracy is up to 98%,and the testing accuracy is about to 90%,so that using LeNet to train MINST is very well.Because this network is primarily designed for MNIST dataset, it performs significantly better on it ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://datahacker.rs/lenet-5-implementation-tensorflow-2-0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
