{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: LeNet on cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)=datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "(32, 32, 3) image shape\n"
     ]
    }
   ],
   "source": [
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_train[0].shape, 'image shape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start using TensorFlow 2.0 in order to build our convolutional neural network. The easiest way to do this is by using the Sequential API. We will wrap it in a class called LeNet. The input is an image, and the output will be a class probability vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet-5 model\n",
    "class LeNet(Sequential):\n",
    "    def __init__(self, input_shape, nb_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.add(Conv2D(6, kernel_size=(5, 5), strides=(1, 1), activation='tanh', input_shape=input_shape, padding=\"same\"))\n",
    "        self.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "        self.add(Conv2D(16, kernel_size=(5, 5), strides=(1, 1), activation='tanh', padding='valid'))\n",
    "        self.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'))\n",
    "        self.add(Flatten())\n",
    "        self.add(Dense(120, activation='tanh'))\n",
    "        self.add(Dense(84, activation='tanh'))\n",
    "        self.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "        self.compile(optimizer='adam',\n",
    "                    loss=categorical_crossentropy,\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"le_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 6)         456       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 16, 16, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 12, 12, 16)        2416      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 83,126\n",
      "Trainable params: 83,126\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LeNet((32, 32,3), 10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place the logs in a timestamped subdirectory\n",
    "# This allows to easy select different training runs\n",
    "# In order not to overwrite some data, it is useful to have a name with a timestamp\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# Specify the callback object\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# tf.keras.callback.TensorBoard ensures that logs are created and stored\n",
    "# We need to pass callback object to the fit method\n",
    "# The way to do this is by passing the list of callback objects, which is in our case just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.6540 - accuracy: 0.4143 - val_loss: 1.5542 - val_accuracy: 0.4418\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.4698 - accuracy: 0.4795 - val_loss: 1.4254 - val_accuracy: 0.4895\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.3661 - accuracy: 0.5177 - val_loss: 1.3873 - val_accuracy: 0.5066\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.3000 - accuracy: 0.5405 - val_loss: 1.3232 - val_accuracy: 0.5302\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.2452 - accuracy: 0.5603 - val_loss: 1.3122 - val_accuracy: 0.5353\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.1966 - accuracy: 0.5761 - val_loss: 1.3056 - val_accuracy: 0.5386\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 1.1516 - accuracy: 0.5934 - val_loss: 1.2978 - val_accuracy: 0.5466\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 1.1158 - accuracy: 0.6062 - val_loss: 1.2853 - val_accuracy: 0.5504\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.0803 - accuracy: 0.6189 - val_loss: 1.2951 - val_accuracy: 0.5473\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.0489 - accuracy: 0.6297 - val_loss: 1.2677 - val_accuracy: 0.5554\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 1.0164 - accuracy: 0.6406 - val_loss: 1.2983 - val_accuracy: 0.5502\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.9885 - accuracy: 0.6533 - val_loss: 1.2998 - val_accuracy: 0.5550\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.9598 - accuracy: 0.6626 - val_loss: 1.2979 - val_accuracy: 0.5566\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.9321 - accuracy: 0.6699 - val_loss: 1.3439 - val_accuracy: 0.5433\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.9040 - accuracy: 0.6820 - val_loss: 1.3449 - val_accuracy: 0.5480\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.8832 - accuracy: 0.6879 - val_loss: 1.3490 - val_accuracy: 0.5479\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.8540 - accuracy: 0.6997 - val_loss: 1.3812 - val_accuracy: 0.5432\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8282 - accuracy: 0.7090 - val_loss: 1.3737 - val_accuracy: 0.5508\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.8082 - accuracy: 0.7162 - val_loss: 1.3803 - val_accuracy: 0.5463\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.7807 - accuracy: 0.7269 - val_loss: 1.4139 - val_accuracy: 0.5439\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdbaf672520>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_test, y_test),callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6008 (pid 30593), started 0:07:59 ago. (Use '!kill 30593' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-359c26888d86af75\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-359c26888d86af75\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is similar with the analysis of the Lenet on MINST, we can see that our own networks are performing bad on cifar10 data sete.The training accuracy is about to 55% even after 20 epoches,and for 5 epoches, the accuracy is only 50%.and the testing accuracy is aslo only about to 50% after 20 epoches,so that using LeNet to train cifar10 is very bad.Because this network is primarily designed for MNIST dataset, it performs significantly better on it ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://bywmm.github.io/2018/12/14/keras%20samples-%20LeNet-5%20on%20cifar10%20dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
